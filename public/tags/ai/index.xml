<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Illia Pantsyr</title>
    <link>https://panilya.github.io/tags/ai/</link>
    <description>Recent content in Ai on Illia Pantsyr</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://panilya.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Could GPU hardware failures limit AI training scaling?</title>
      <link>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</link>
      <pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</guid>
      <description>&lt;p&gt;A fresh analytical &lt;a href=&#34;https://epoch.ai/blog/hardware-failures-wont-limit-ai-scaling&#34;&gt;report&lt;/a&gt; from Epoch AI on the continued scaling of training capacity. In this work, they attempt to answer the following question: to what extent is scaling limited by hardware issues?&lt;/p&gt;&#xA;&lt;p&gt;Graphics processing units (GPUs) can fail during training for various reasons: memory damage, disconnections/restarts, or network issues. Even a single slightly slowed-down GPU can become a bottleneck for the entire system if not replaced promptly.&lt;/p&gt;&#xA;&lt;p&gt;When Meta trained its largest model, Llama 3.1 with 405B parameters, on 16,000 GPUs, there were over 400 hardware failures over 54 daysâ€”roughly one every three hours. If this is scaled up to setups with over a million GPUs, failures would occur every few minutes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper summary | Lost in the Middle: How Language Models Use Long Contexts</title>
      <link>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</guid>
      <description>&lt;p&gt;Almost 1 year ago, &lt;a href=&#34;https://arxiv.org/abs/2307.03172&#34;&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt; paper was published that empirically investigated the effectiveness of finding information in the input context depending on the location of the correct answer in the context among other relevant information. The researchers used &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo-16k&lt;/code&gt;, &lt;code&gt;Claude 1.3&lt;/code&gt;, &lt;code&gt;Claude 1.3 (100k)&lt;/code&gt;, &lt;code&gt;MPT-30B-Instruct&lt;/code&gt; and &lt;code&gt;LongChat-13B (16k)&lt;/code&gt; and found that the quality jumps A LOT when you change the position of a piece of text with the correct answer among other relevant text that does not contain the answer to the question.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
