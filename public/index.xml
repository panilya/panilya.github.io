<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Illia Pantsyr</title>
    <link>https://panilya.github.io/</link>
    <description>Recent content on Illia Pantsyr</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://panilya.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Could GPU hardware failures limit AI training scaling?</title>
      <link>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</link>
      <pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</guid>
      <description>&lt;p&gt;A fresh analytical &lt;a href=&#34;https://epoch.ai/blog/hardware-failures-wont-limit-ai-scaling&#34;&gt;report&lt;/a&gt; from Epoch AI on the continued scaling of training capacity. In this work, they attempt to answer the following question: to what extent is scaling limited by hardware issues?&lt;/p&gt;&#xA;&lt;p&gt;Graphics processing units (GPUs) can fail during training for various reasons: memory damage, disconnections/restarts, or network issues. Even a single slightly slowed-down GPU can become a bottleneck for the entire system if not replaced promptly.&lt;/p&gt;&#xA;&lt;p&gt;When Meta trained its largest model, Llama 3.1 with 405B parameters, on 16,000 GPUs, there were over 400 hardware failures over 54 days‚Äîroughly one every three hours. If this is scaled up to setups with over a million GPUs, failures would occur every few minutes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper summary | Lost in the Middle: How Language Models Use Long Contexts</title>
      <link>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</guid>
      <description>&lt;p&gt;Almost 1 year ago, &lt;a href=&#34;https://arxiv.org/abs/2307.03172&#34;&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt; paper was published that empirically investigated the effectiveness of finding information in the input context depending on the location of the correct answer in the context among other relevant information. The researchers used &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo-16k&lt;/code&gt;, &lt;code&gt;Claude 1.3&lt;/code&gt;, &lt;code&gt;Claude 1.3 (100k)&lt;/code&gt;, &lt;code&gt;MPT-30B-Instruct&lt;/code&gt; and &lt;code&gt;LongChat-13B (16k)&lt;/code&gt; and found that the quality jumps A LOT when you change the position of a piece of text with the correct answer among other relevant text that does not contain the answer to the question.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deployment Strategies</title>
      <link>https://panilya.github.io/posts/deployment_strategies/</link>
      <pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/deployment_strategies/</guid>
      <description>&lt;p&gt;Deployment strategies provide a systematic approach to releasing software changes, minimizing risks, and maintaining consistency across projects and teams.&#xA;Without a well-defined strategy and systematic approach, deployments can lead to downtime, data loss, or system failures, resulting in frustrated users and revenue loss.&#xA;Before we start exploring different deployment strategies in more detail, let‚Äôs take a look at the short overview of each deployment strategy mentioned in this article:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All-at-once deployment: This strategy involves updating all the target environments at once, making it the fastest but riskiest approach.&lt;/li&gt;&#xA;&lt;li&gt;In-place deployment: This involves stopping the current application and replacing it with a new version, directly affecting availability.&lt;/li&gt;&#xA;&lt;li&gt;Blue/Green deployment: A zero-downtime approach that involves running two identical environments and switching from old to new.&lt;/li&gt;&#xA;&lt;li&gt;Canary deployment: Introduces new changes incrementally to a small subset of users before a full rollout.&lt;/li&gt;&#xA;&lt;li&gt;Shadow deployment: Mirrors real traffic to a shadow environment where the new deployment is tested without affecting the live environment.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;all-at-once-deployment&#34;&gt;All-at-once deployment&lt;/h2&gt;&#xA;&lt;p&gt;All-at-once deployment strategy, also known as the &amp;ldquo;Big Bang&amp;rdquo; deployment strategy, involves simultaneously releasing your application&amp;rsquo;s new version to all servers or environments. This method is straightforward and can be implemented quickly, as it does not require complex orchestration or additional infrastructure. The primary benefit of this approach is its simplicity and the ability to immediately transition all users to the new version of the application.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ways to reduce JVM Docker image size</title>
      <link>https://panilya.github.io/posts/ways_to_reduce_jvm_docker_image_size/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/ways_to_reduce_jvm_docker_image_size/</guid>
      <description>&lt;p&gt;This blog post focuses on optimizing the size of JVM Docker images. It explores various techniques such as multi-stage builds, jlink, jdeps, and experimenting with base images. By implementing these optimizations, deployments can be faster and resource usage can be optimized.&lt;/p&gt;&#xA;&lt;h3 id=&#34;the-problem&#34;&gt;The problem&lt;/h3&gt;&#xA;&lt;p&gt;Since Java 11, there is no pre-bundled JRE provided. As a result, basic Dockerfiles without any optimization can result in large image sizes. In the absence of a provided JRE, it becomes necessary to explore techniques and optimizations to reduce the size of JVM Docker images.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Datafaker 2.0</title>
      <link>https://panilya.github.io/posts/datafaker_2_0/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/datafaker_2_0/</guid>
      <description>&lt;p&gt;One of the major changes in Datafaker 2.0 is the requirement for Java version 17 as a minimum, similar to popular frameworks such as Spring Boot 3.0. This release of Datafaker brings significant improvements to the library&amp;rsquo;s performance, support of Java Records, capability to generate larger amounts of test data, and much more.&lt;/p&gt;&#xA;&lt;h2 id=&#34;schemas-and-transformers&#34;&gt;Schemas and transformers&lt;/h2&gt;&#xA;&lt;p&gt;The most common use case for Datafaker is to generate random readable values in Java, such as firstnames, quotes or other values.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Class Loaders in JVM: An Overview</title>
      <link>https://panilya.github.io/posts/class_loaders_overview/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/posts/class_loaders_overview/</guid>
      <description>&lt;p&gt;Class loaders are an essential part of the Java Virtual Machine (JVM), but many developers consider them to be mysterious. This article aims to demystify the subject by providing a basic understanding of how class loading works in the JVM.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-are-classloaders&#34;&gt;What are classloaders&lt;/h2&gt;&#xA;&lt;p&gt;In the Java Virtual Machine (JVM), classes are loaded dynamically and found through a process called class loading. Class loading is the process of loading a class from its binary representation (usually a .class file) into memory so that it can be executed by the JVM. This is where we need classloaders. Class loaders are used to load .class file into the memory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hello üëãüèº</title>
      <link>https://panilya.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://panilya.github.io/about/</guid>
      <description>&lt;p&gt;My name is Illia. Over the last few years of commercial experience I&amp;rsquo;ve worked with a wide variety of technologies. My main interests are Backend systems and Generative AI. Some of the technologies I worked the most are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Java, Spring Boot, Spring Data JPA/JDBC, Spring Security, Hibernate&lt;/li&gt;&#xA;&lt;li&gt;Python, LlamaIndex, FastAPI&lt;/li&gt;&#xA;&lt;li&gt;Large Language Models, Generative AI&lt;/li&gt;&#xA;&lt;li&gt;Kotlin on server-side&lt;/li&gt;&#xA;&lt;li&gt;React, TypeScript, NextJS, NodeJS&lt;/li&gt;&#xA;&lt;li&gt;Relational databases, MongoDB, Redis, Vector databases&lt;/li&gt;&#xA;&lt;li&gt;AWS, Docker&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve always been fascinated by Open-Source projects and its community and were participating in it way before my first commercial experience. One of the Open-Source project I‚Äôm proud to be involved in is &lt;a href=&#34;https://github.com/datafaker-net/datafaker&#34;&gt;Datafaker&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
