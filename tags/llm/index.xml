<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>llm on Illia Pantsyr</title><link>https://panilya.github.io/tags/llm/</link><description>Recent content in llm on Illia Pantsyr</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 23 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://panilya.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Could GPU hardware failures limit AI training scaling?</title><link>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</guid><description>A fresh analytical report from Epoch AI on the continued scaling of training capacity. In this work, they attempt to answer the following question: to what extent is scaling limited by hardware issues?
Graphics processing units (GPUs) can fail during training for various reasons: memory damage, disconnections/restarts, or network issues. Even a single slightly slowed-down GPU can become a bottleneck for the entire system if not replaced promptly.
When Meta trained its largest model, Llama 3.</description></item><item><title>Paper summary | Lost in the Middle: How Language Models Use Long Contexts</title><link>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</link><pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</guid><description>Almost 1 year ago, Lost in the Middle: How Language Models Use Long Contexts paper was published that empirically investigated the effectiveness of finding information in the input context depending on the location of the correct answer in the context among other relevant information. The researchers used gpt-3.5-turbo, gpt-3.5-turbo-16k, Claude 1.3, Claude 1.3 (100k), MPT-30B-Instruct and LongChat-13B (16k) and found that the quality jumps A LOT when you change the position of a piece of text with the correct answer among other relevant text that does not contain the answer to the question.</description></item></channel></rss>