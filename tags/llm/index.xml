<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on Illia Pantsyr</title><link>https://panilya.github.io/tags/llm/</link><description>Recent content in Llm on Illia Pantsyr</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 03 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://panilya.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Paper summary | Lost in the Middle: How Language Models Use Long Contexts</title><link>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</link><pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</guid><description>Almost 1 year ago, Lost in the Middle: How Language Models Use Long Contexts paper was published that empirically investigated the effectiveness of finding information in the input context depending on the location of the correct answer in the context among other relevant information. The researchers used gpt-3.5-turbo, gpt-3.5-turbo-16k, Claude 1.3, Claude 1.3 (100k), MPT-30B-Instruct and LongChat-13B (16k) and found that the quality jumps A LOT when you change the position of a piece of text with the correct answer among other relevant text that does not contain the answer to the question.</description></item></channel></rss>