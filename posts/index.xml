<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Illia Pantsyr</title><link>https://panilya.github.io/posts/</link><description>Recent content in Posts on Illia Pantsyr</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 04 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://panilya.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Domain-Specific Large Language Models as the Next Milestone</title><link>https://panilya.github.io/posts/domain_specific_large_language_models_as_the_next_milestone/</link><pubDate>Sun, 04 May 2025 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/domain_specific_large_language_models_as_the_next_milestone/</guid><description>The rapid evolution of Generative AI, particularly Large Language Models (LLMs), has reshaped many aspects of technology and industry. As general-purpose models like GPT-4o, Claude, and Gemini demonstrate increasingly sophisticated capabilities, the question arises: what is the next major milestone in this field? While advancements continue on multiple fronts, a compelling case can be made for Domain-Specific Large Language Models (DLLMs) representing the next significant leap forward. This post explores why specializing LLMs for particular fields might be the crucial next step in unlocking their full potential.</description></item><item><title>12 essential client management rules</title><link>https://panilya.github.io/posts/12_essential_client_management_rules/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/12_essential_client_management_rules/</guid><description>Below are my notes from the insightful presentation &amp;ldquo;Secrets to Optimal Client Service, With Jim Donovan&amp;rdquo; by the University of Virginia School of Law.
Never use jargon Clients likely won&amp;rsquo;t understand technical terms, and they&amp;rsquo;re even less likely to admit when they don&amp;rsquo;t understand. This makes them feel foolish and they&amp;rsquo;ll blame you for it.
Use pauses Pauses create opportunities for clients to ask questions or provide comments. Pauses also help you slow down, appear less anxious and more composed.</description></item><item><title>Anthropic demonstrated how AI can analyze itself</title><link>https://panilya.github.io/posts/anthropic_demonstrated_how_ai_can_analyze_itself/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/anthropic_demonstrated_how_ai_can_analyze_itself/</guid><description>Yesterday, Anthropic published an amazing study—they developed a system called Clio, which can safely analyze millions of conversations with the AI assistant Claude.
A New Perspective on AI Usage Clio allows for examining real-life scenarios of AI usage in everyday life while preserving user privacy by working only with aggregated data. Anthropic has revealed fascinating trends:
Most popular tasks: Users primarily rely on AI for programming, content creation, and research. Cultural differences: AI usage varies by region; for instance, in Japan, discussions often center on societal challenges like aging populations.</description></item><item><title>Could GPU hardware failures limit AI training scaling?</title><link>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/could_gpu_hardware_failures_limit_ai_training_scaling/</guid><description>A fresh analytical report from Epoch AI on the continued scaling of training capacity. In this work, they attempt to answer the following question: to what extent is scaling limited by hardware issues?
Graphics processing units (GPUs) can fail during training for various reasons: memory damage, disconnections/restarts, or network issues. Even a single slightly slowed-down GPU can become a bottleneck for the entire system if not replaced promptly.
When Meta trained its largest model, Llama 3.</description></item><item><title>Paper summary | Lost in the Middle: How Language Models Use Long Contexts</title><link>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</link><pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/lost_in_the_middle_how_language_models_use_long_contexts/</guid><description>Almost 1 year ago, Lost in the Middle: How Language Models Use Long Contexts paper was published that empirically investigated the effectiveness of finding information in the input context depending on the location of the correct answer in the context among other relevant information. The researchers used gpt-3.5-turbo, gpt-3.5-turbo-16k, Claude 1.3, Claude 1.3 (100k), MPT-30B-Instruct and LongChat-13B (16k) and found that the quality jumps A LOT when you change the position of a piece of text with the correct answer among other relevant text that does not contain the answer to the question.</description></item><item><title>Deployment Strategies</title><link>https://panilya.github.io/posts/deployment_strategies/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/deployment_strategies/</guid><description>Deployment strategies provide a systematic approach to releasing software changes, minimizing risks, and maintaining consistency across projects and teams. Without a well-defined strategy and systematic approach, deployments can lead to downtime, data loss, or system failures, resulting in frustrated users and revenue loss. Before we start exploring different deployment strategies in more detail, let’s take a look at the short overview of each deployment strategy mentioned in this article:
All-at-once deployment: This strategy involves updating all the target environments at once, making it the fastest but riskiest approach.</description></item><item><title>Ways to reduce JVM Docker image size</title><link>https://panilya.github.io/posts/ways_to_reduce_jvm_docker_image_size/</link><pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/ways_to_reduce_jvm_docker_image_size/</guid><description>This blog post focuses on optimizing the size of JVM Docker images. It explores various techniques such as multi-stage builds, jlink, jdeps, and experimenting with base images. By implementing these optimizations, deployments can be faster and resource usage can be optimized.
The problem Since Java 11, there is no pre-bundled JRE provided. As a result, basic Dockerfiles without any optimization can result in large image sizes. In the absence of a provided JRE, it becomes necessary to explore techniques and optimizations to reduce the size of JVM Docker images.</description></item><item><title>Datafaker 2.0</title><link>https://panilya.github.io/posts/datafaker_2_0/</link><pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/datafaker_2_0/</guid><description>One of the major changes in Datafaker 2.0 is the requirement for Java version 17 as a minimum, similar to popular frameworks such as Spring Boot 3.0. This release of Datafaker brings significant improvements to the library&amp;rsquo;s performance, support of Java Records, capability to generate larger amounts of test data, and much more.
Schemas and transformers The most common use case for Datafaker is to generate random readable values in Java, such as firstnames, quotes or other values.</description></item><item><title>Class Loaders in JVM: An Overview</title><link>https://panilya.github.io/posts/class_loaders_overview/</link><pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate><guid>https://panilya.github.io/posts/class_loaders_overview/</guid><description>Class loaders are an essential part of the Java Virtual Machine (JVM), but many developers consider them to be mysterious. This article aims to demystify the subject by providing a basic understanding of how class loading works in the JVM.
What are classloaders In the Java Virtual Machine (JVM), classes are loaded dynamically and found through a process called class loading. Class loading is the process of loading a class from its binary representation (usually a .</description></item></channel></rss>